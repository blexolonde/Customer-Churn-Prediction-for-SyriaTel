# Customer Churn Prediction for SyriaTel

## Project Overview
This project aims to predict customer churn for SyriaTel, a telecommunications company. Churn prediction is crucial for reducing customer loss and improving retention strategies. The goal is to build a classification model that identifies at-risk customers before they leave the service.

## ðŸ“‚ Dataset
The dataset contains customer usage patterns and service-related features. The key target variable is **Churn**, which indicates whether a customer has left the service.

## ðŸŽ¯ Objective
- Develop and evaluate machine learning models to predict customer churn.
- Identify key features that influence churn.
- Provide actionable insights to mitigate customer churn.

---
##  Data Preprocessing
- **Feature Engineering**: Combined and transformed categorical and numerical features.
- **Handling Imbalance**: Used **SMOTE (Synthetic Minority Over-sampling Technique)** to address class imbalance.
- **Encoding & Scaling**: Applied **one-hot encoding** for categorical variables and **MinMax scaling** for numerical features.

---
##  Machine Learning Models
Five classification models were trained and optimized using **hyperparameter tuning**:

| Model                   | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------------------------|----------|------------|--------|----------|---------|
| **Random Forest**       | 0.8951   | 0.6233     | 0.6943 | 0.6569   | 0.8625  |
| **LightGBM**           | 0.8943   | 0.6287     | 0.6580 | 0.6430   | 0.8460  |
| **XGBoost**            | 0.8688   | 0.5388     | 0.6477 | 0.5882   | 0.8338  |
| **Decision Tree**      | 0.8148   | 0.4075     | 0.6166 | 0.4907   | 0.7615  |
| **Logistic Regression** | 0.7031   | 0.3037     | 0.8135 | 0.4423   | 0.8179  |

### Best Hyperparameters for Each Model
Hyperparameters were chosen using **Grid Search and Random Search** techniques, optimizing for **F1-score and ROC-AUC** to balance precision and recall.

- **Logistic Regression**: solver='liblinear', penalty='l1', C=0.1
  - Chosen to encourage feature selection via L1 regularization and handle multicollinearity.
- **Decision Tree**: min_samples_split=5, min_samples_leaf=2, max_depth=None
  - Optimized for better generalization and reduced overfitting.
- **Random Forest**: n_estimators=500, min_samples_split=2, min_samples_leaf=1, max_depth=None
  - A higher number of estimators improves stability, while default depth allows deeper trees.
- **LightGBM**: subsample=0.7, num_leaves=40, n_estimators=500, learning_rate=0.1, boosting_type='gbdt'
  - Balanced between fast training and preventing overfitting with controlled leaf growth.
- **XGBoost**: subsample=0.8, n_estimators=500, max_depth=6, learning_rate=0.2
  - Tuned for optimal feature interactions while maintaining computational efficiency.

---
## ðŸ“ˆ Model Evaluation: Confusion Matrices
Below are the confusion matrices for each model:

![Confusion Matrices](Untitled.png)

###  Interpretation:
- **Random Forest & LightGBM** performed the best in terms of **F1-score and ROC-AUC**, meaning they balance precision and recall well.
- **Logistic Regression** has the highest recall, meaning it captures the most churn cases but at the cost of precision.
- **Decision Tree** and **XGBoost** are decent but slightly underperform compared to the top models.
- **False Positives & False Negatives**:
  - **Random Forest & LightGBM** have the lowest false negatives, making them the best candidates for predicting churn.
  - **Logistic Regression** misclassifies many customers as churn who are not actually leaving.

---
## ðŸ“‰ ROC Curve Analysis
Below is the ROC Curve comparing model performance:

![ROC Curve](Untitled.png)

###  Interpretation:
- **Random Forest (AUC = 0.86) and LightGBM (AUC = 0.85)** perform best, showing strong predictive power.
- **XGBoost (AUC = 0.83)** follows closely, indicating good performance.
- **Logistic Regression (AUC = 0.82)** has decent performance but struggles with precision.
- **Decision Tree (AUC = 0.76)** performs the worst, meaning it is less reliable for churn prediction.

---
##  Business Impact & Recommendations
Based on the findings, the following actions are recommended:
- **Customer Retention Programs**: Target customers identified as high churn risk with personalized offers or discounts.
- **Improve Customer Support**: Customers with high service usage variations may require better support.
- **Feature Optimization**: Focus on the most important features affecting churn to enhance prediction accuracy.
- **Threshold Tuning**: Adjust classification thresholds to balance false positives and false negatives based on business needs.

---
## ðŸ”§ Future Work
- Incorporate deep learning techniques (e.g., Neural Networks).
- Use advanced feature selection methods to improve model interpretability.
- Conduct A/B testing on retention strategies using the model predictions.

---
## ðŸ“Œ Conclusion
This project successfully developed a robust **customer churn prediction model** using multiple machine learning techniques. **Random Forest and LightGBM** emerged as the best models, providing a strong balance between precision and recall. Future work can focus on further refining predictions and deploying the model into a production environment for real-time monitoring.

---
## ðŸ“¬ Contact
For any inquiries or collaboration opportunities, feel free to reach out!

ðŸ“§ Email: olonde.blex@gmail.com
ðŸ”— LinkedIn: [blex/olonde]
